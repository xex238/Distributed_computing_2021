# -*- coding: utf-8 -*-
"""for_Sava2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZcnFKaU8z4E7PYAa2HOABY9Fd8pE40a3
"""

import os
import torch
import torch.nn.functional as F

from torch import nn

import time
from torch import optim
from torch.utils import data

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold, TimeSeriesSplit
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tqdm.notebook import tqdm
from copy import deepcopy

df = pd.read_csv('ratings.csv').\
                 sort_values('ts')[['user_uid', 'element_uid', 'rating']]

def transform_data(df):
    '''
        Преобразуем номера users и items в диапозон от 0 до n,
        чтобы этими числами легче было оперировать дальше.
    '''
    label_encoder = LabelEncoder()
    
    label_encoder.fit(df.user_uid)
    df.user_uid = label_encoder.transform(df.user_uid)
    
    label_encoder.fit(df.element_uid)
    df.element_uid = label_encoder.transform(df.element_uid)
    
    return df

df = transform_data(df)

df_train, df_test = train_test_split(df)

class MySVD(nn.Module):
    def __init__(self, user_size, item_size, empedding_size):
        super(MySVD, self).__init__()
        self.user_embedding = nn.Embedding(user_size, empedding_size)
        self.item_embedding = nn.Embedding(item_size, empedding_size)
        self.bu = nn.Embedding(user_size, 1)
        self.bi = nn.Embedding(item_size, 1)
        
    def forward(self, user, item):
        embedd_user = self.user_embedding(user)
        embedd_item = self.item_embedding(item)
        bu = self.bu(user).squeeze()
        bi = self.bi(item).squeeze()
        return 8.18907905831947 + bu + bi + torch.sum(embedd_user*embedd_item, dim=1)

class User_Item_Set(object):
    def __init__(self, df):
        self.df = df

    def __getitem__(self, idx):
        user, item, rating = self.df.iloc[idx]
        return user, item, rating

    def __len__(self):
        return len(self.df)

def collate_fn(batch):
    user = torch.LongTensor([item[0] for item in batch])
    item = torch.LongTensor([item[1] for item in batch])
    rating = torch.Tensor([item[2] for item in batch])
    return user, item, rating

test_set = User_Item_Set(df_test)
train_set = User_Item_Set(df_train)

test_dataloader = data.DataLoader(test_set, batch_size=300, collate_fn=collate_fn)
train_dataloader = data.DataLoader(train_set, batch_size=300, collate_fn=collate_fn, shuffle=True)

def perform_one_epoch(model, optimizer, criterion, loader, is_train=True):
    ''' 
        функция прогоняет одну эпоху
        одновременно и train_epoch, и evaluate 
    '''
    if is_train:
        model.train()
    else:
        model.eval()
        
    running_loss = 0.0
    total = 0.0
  
    for i, (user, item, rating) in enumerate(tqdm(loader)):
        user = user.to(device)
        item = item.to(device)
        rating = rating.to(device)
        
        answers = model(user, item)
        loss = criterion(answers, rating)
        #loss = loss_/user.data.size(0)
        
        if is_train:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
  
        running_loss += loss.item()
        total += user.data.size(0)
    return running_loss/total

def train_model(model, epoch_num, optimizer, criterion, trainloader, testloader, early_stopping=3, verbose=0):
    l_train_losses = []
    l_test_losses = []
    for epoch in range(epoch_num):
        loss_train = perform_one_epoch(model, optimizer, criterion, trainloader, is_train=True)
        l_train_losses.append(loss_train)
        
        loss_test = perform_one_epoch(model, optimizer, criterion, testloader, is_train=False)
        l_test_losses.append(loss_test)
        if verbose:
            if (epoch+1)%verbose==0:
                print(f'train loss epoch {epoch+1}: {loss_train}')
                print(f'test loss epoch {epoch+1}: {loss_test}')
                print('----------------------')
        
        if epoch > early_stopping: 
            if np.all(np.array(l_test_losses)[-early_stopping-1:-1] < loss_test):
                if verbose:
                    print(f'train loss epoch {epoch+1}: {loss_train}')
                    print(f'test loss epoch {epoch+1}: {loss_test}')
                    print('early stopping!')
                break
    return l_train_losses, l_test_losses

user_size = df.user_uid.max()+1
item_size = df.element_uid.max()+1

# задаем параметры обучения
model_svd = MySVD(user_size, item_size, 8)
criterion_svd = nn.MSELoss(reduction='sum')
optimizer_svd = optim.SGD(model_svd.parameters(), lr=0.01, weight_decay=0.1)
epoch_num_svd = 4
# обучаем модель, качество MSE (без корня)
train_model(model_svd, epoch_num_svd,
            optimizer_svd, criterion_svd,
            train_dataloader, test_dataloader,
            early_stopping=10, verbose=1)

model_svd.user_embedding.weight.grad