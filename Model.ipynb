{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "340c2e5c-a9d1-4412-bb73-32ceb07a2afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 32, 32, 32)       128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 64)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               4194560   \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 256)              1024      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 128)              512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,250,058\n",
      "Trainable params: 4,249,098\n",
      "Non-trainable params: 960\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Set random seed for purposes of reproducibility\n",
    "seed = 21\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "# loading in the data\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# normalize the inputs from 0-255 to between 0 and 1 by dividing by 255\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "class_num = y_test.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256, kernel_constraint=maxnorm(3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, kernel_constraint=maxnorm(3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(class_num))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "epochs = 1\n",
    "optimizer = 'adam'\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "#print(model.summary())\n",
    "\n",
    "#numpy.random.seed(seed)\n",
    "#model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c5ef2-59f5-402d-84a3-411540334a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bdaf1e-47e0-4bc1-b796-cb96c11dbf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_by_one_sample(model, x, y):\n",
    "  with tf.GradientTape() as tape:\n",
    "    pred = model(x[None, :])\n",
    "    loss_by_one_sample = (y[None, :] - pred)**2\n",
    "    grads = tape.gradient(loss_by_one_sample, model.trainable_variables)\n",
    "  return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b281d-0303-4cdc-b8ed-6ca658df1cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0aa184-6e69-4687-960f-5993519ba39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_one_epoch(model, opt, X_train, y_train, X_test, y_test, batch_size):\n",
    "\n",
    "  for i_start, i_stop in zip(range(0, len(X_train) - batch_size, batch_size),\n",
    "                             range(batch_size, len(X_train), batch_size)):\n",
    "    \n",
    "    def lambda_grad(var):\n",
    "      return grad_by_one_sample(model, *var)\n",
    "    l_grads = list(map(lambda_grad,zip(X_train[i_start: i_stop],\n",
    "                                           y_train[i_start: i_stop])))\n",
    "    grads = [tf.zeros_like(g) for g in l_grads[0]]\n",
    "    for i in range(len(grads)):\n",
    "      for j in range(len(l_grads)):\n",
    "        grads[i] += l_grads[j][i]\n",
    "    for i in range(len(grads)):\n",
    "      grads[i] = grads[i] / len(l_grads)\n",
    "\n",
    "    grads_list.append(grads)\n",
    "    opt.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  test_pred = model(X_test)\n",
    "  test_loss = tf.reduce_mean((y_test - test_pred)**2)\n",
    "  return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c468c07-9e0e-49d6-aa68-a63a3d07170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                opt,\n",
    "                X_train,\n",
    "                y_train,\n",
    "                X_test,\n",
    "                y_test,\n",
    "                batch_size,\n",
    "                epoch_num = 10):\n",
    "  for epoch in range(epoch_num):\n",
    "    test_loss = perform_one_epoch(model, opt, X_train, y_train, X_test, y_test, batch_size)\n",
    "    print(f'epoch: {epoch}, test_loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1160ede-2581-49a8-afeb-0e4a9aa810dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model,\n",
    "            opt,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            batch_size=16,\n",
    "            epoch_num=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4bfa33-7bc6-4522-969e-a89229a13f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
